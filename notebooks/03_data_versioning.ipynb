{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7d2545",
   "metadata": {},
   "source": [
    "# Data Versioning with DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2694423a",
   "metadata": {},
   "source": [
    "The objective of this notebook is to set up DVC for data control and document all dataset changes\n",
    "\n",
    "Contents:\n",
    "1. DVC Initialization Scripts\n",
    "2. Dataset Inventory\n",
    "3. Version History Documentation\n",
    "4. Change Log Creation\n",
    "5. DVC Workflow Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc59ec",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02daf556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA VERSIONING WITH DVC\n",
      "================================================================================\n",
      "\n",
      "Project Root: /Users/lia/Desktop/Fase1\n",
      "Data Raw: /Users/lia/Desktop/Fase1/data/raw\n",
      "Data Processed: /Users/lia/Desktop/Fase1/data/processed\n",
      "Scripts: /Users/lia/Desktop/Fase1/scripts\n",
      "Reports: /Users/lia/Desktop/Fase1/reports\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Define project paths\n",
    "ROOT = Path.cwd().parent if (Path.cwd().parent / 'data').exists() else Path.cwd()\n",
    "DATA_RAW = ROOT / \"data\" / \"raw\"\n",
    "DATA_PROC = ROOT / \"data\" / \"processed\"\n",
    "SCRIPTS = ROOT / \"scripts\"\n",
    "REPORTS = ROOT / \"reports\"\n",
    "\n",
    "# Create directories\n",
    "for p in [DATA_RAW, DATA_PROC, SCRIPTS, REPORTS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"DATA VERSIONING WITH DVC\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nProject Root: {ROOT}\")\n",
    "print(f\"Data Raw: {DATA_RAW}\")\n",
    "print(f\"Data Processed: {DATA_PROC}\")\n",
    "print(f\"Scripts: {SCRIPTS}\")\n",
    "print(f\"Reports: {REPORTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ec366",
   "metadata": {},
   "source": [
    "## Create DVC Initialization Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c16e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DVC initialization script created: /Users/lia/Desktop/Fase1/scripts/init_dvc.sh\n",
      "Usage: ./scripts/init_dvc.sh\n"
     ]
    }
   ],
   "source": [
    "# Create init_dvc.sh script\n",
    "init_dvc_script = \"\"\"#!/bin/bash\n",
    "# DVC Initialization Script\n",
    "# This script sets up DVC for data version control\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"==================================================\"\n",
    "echo \"INITIALIZING DVC FOR DATA VERSION CONTROL\"\n",
    "echo \"==================================================\"\n",
    "\n",
    "# Check if DVC is installed\n",
    "if ! command -v dvc &> /dev/null; then\n",
    "    echo \"ERROR: DVC is not installed\"\n",
    "    echo \"Install with: pip install dvc\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Check if git repository exists\n",
    "if [ ! -d .git ]; then\n",
    "    echo \"ERROR: Not a git repository\"\n",
    "    echo \"Initialize git first: git init && git add . && git commit -m 'init repo'\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Initialize DVC\n",
    "echo \"\"\n",
    "echo \"Step 1: Initializing DVC...\"\n",
    "dvc init\n",
    "\n",
    "# Check if initialization was successful\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"✓ DVC initialized successfully\"\n",
    "else\n",
    "    echo \"DVC initialization failed\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"Step 2: Configuring DVC...\"\n",
    "# Set autostage to true (automatically stage DVC files)\n",
    "dvc config core.autostage true\n",
    "echo \"✓ DVC configuration complete\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"Step 3: Adding DVC files to git...\"\n",
    "git add .dvc .dvcignore || true\n",
    "git commit -m \"chore: init dvc\" || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"Step 4: Add a local remote for storage\"\n",
    "mkdir -p ../dvcstore\n",
    "dvc remote add -d localstore ../dvcstore || true\n",
    "git add .dvc/config || true\n",
    "git commit -m \"chore: add local dvc remote\" || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"==================================================\"\n",
    "echo \" DVC INITIALIZATION COMPLETE\"\n",
    "echo \"==================================================\"\n",
    "echo \"\"\n",
    "echo \"Next steps:\"\n",
    "echo \"1. Put raw data in: data/raw/\"\n",
    "echo \"2. Track raw dir: dvc add data/raw\"\n",
    "echo \"3. Commit pointer: git add data/raw.dvc && git commit -m 'track raw data'\"\n",
    "echo \"4. Push data: dvc push\"\n",
    "echo \"5. Define pipeline outs in dvc.yaml for data/processed, models, reports/figures\"\n",
    "\"\"\"\n",
    "\n",
    "init_path = SCRIPTS / \"init_dvc.sh\"\n",
    "init_path.write_text(init_dvc_script, encoding=\"utf-8\")\n",
    "os.chmod(init_path, 0o755)\n",
    "\n",
    "print(f\"\\n✓ DVC initialization script created: {init_path}\")\n",
    "print(f\"Usage: ./scripts/init_dvc.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299aa3b2",
   "metadata": {},
   "source": [
    "## Create DVC Data Tracking Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "080fc5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DVC tracking script created: /Users/lia/Desktop/Fase1/scripts/track_data_dvc.sh\n",
      "Usage: ./scripts/track_data_dvc.sh\n"
     ]
    }
   ],
   "source": [
    "# Create track_data_dvc.sh script\n",
    "track_data_script = \"\"\"#!/bin/bash\n",
    "set -euo pipefail\n",
    "\n",
    "# DVC Data Tracking Script\n",
    "# Tracks raw data files with DVC\n",
    "\n",
    "echo \"==================================================\"\n",
    "echo \"ADDING RAW DATA FILES TO DVC TRACKING\"\n",
    "echo \"==================================================\"\n",
    "\n",
    "RAW_DIR=\"data/raw\"\n",
    "\n",
    "if [ -d \"$RAW_DIR\" ]; then\n",
    "    echo \"Tracking $RAW_DIR ...\"\n",
    "    dvc add \"$RAW_DIR\"\n",
    "    echo \"✓ Added $RAW_DIR to DVC\"\n",
    "else\n",
    "    echo \"ERROR: $RAW_DIR not found. Create it and place your raw files there.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"Adding .dvc pointers to git...\"\n",
    "git add data/raw.dvc .gitignore || true\n",
    "git commit -m \"chore: track raw data with DVC\" || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"Pushing raw data to DVC remote...\"\n",
    "dvc push || true\n",
    "\n",
    "echo \"\"\n",
    "echo \"==================================================\"\n",
    "echo \" DATA TRACKING COMPLETE\"\n",
    "echo \"==================================================\"\n",
    "echo \"\"\n",
    "echo \"NOTE: Processed data, models, and figures should be pipeline outs in dvc.yaml\"\n",
    "echo \"Run the pipeline with: dvc repro\"\n",
    "\"\"\"\n",
    "\n",
    "track_path = SCRIPTS / \"track_data_dvc.sh\"\n",
    "track_path.write_text(track_data_script, encoding=\"utf-8\")\n",
    "os.chmod(track_path, 0o755)\n",
    "\n",
    "print(f\"\\n✓ DVC tracking script created: {track_path}\")\n",
    "print(f\"Usage: ./scripts/track_data_dvc.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d88217",
   "metadata": {},
   "source": [
    "## Create dvc.yaml Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3f3f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ DVC pipeline created: /Users/lia/Desktop/Fase1/dvc.yaml\n",
      "✓ Parameters file created: /Users/lia/Desktop/Fase1/params.yaml\n"
     ]
    }
   ],
   "source": [
    "dvc_yaml = \"\"\"stages:\n",
    "  preprocess:\n",
    "    cmd: python notebooks/02_data_preprocessing.py\n",
    "    deps:\n",
    "      - notebooks/02_data_preprocessing.py\n",
    "      - data/processed/student_entry_performance_eda.csv\n",
    "    params:\n",
    "      - preprocessing.test_size\n",
    "      - preprocessing.random_state\n",
    "    outs:\n",
    "      - data/processed/student_performance_cleaned.csv:\n",
    "          persist: true\n",
    "      - data/processed/student_performance_preprocessed.csv:\n",
    "          persist: true\n",
    "      - data/processed/student_performance_train.csv:\n",
    "          persist: true\n",
    "      - data/processed/student_performance_test.csv:\n",
    "          persist: true\n",
    "      - data/processed/feature_names.txt:\n",
    "          persist: true\n",
    "      - reports/figures:\n",
    "          persist: true\n",
    "\"\"\"\n",
    "\n",
    "dvc_yaml_path = ROOT / \"dvc.yaml\"\n",
    "dvc_yaml_path.write_text(dvc_yaml, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n✓ DVC pipeline created: {dvc_yaml_path}\")\n",
    "\n",
    "# Create params.yaml\n",
    "params_yaml = \"\"\"preprocessing:\n",
    "  test_size: 0.2\n",
    "  random_state: 42\n",
    "  scaler: \"standard\"\n",
    "\n",
    "model:\n",
    "  algorithm: \"SVM\"\n",
    "  random_state: 42\n",
    "\"\"\"\n",
    "\n",
    "params_yaml_path = ROOT / \"params.yaml\"\n",
    "params_yaml_path.write_text(params_yaml, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✓ Parameters file created: {params_yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2d5f2",
   "metadata": {},
   "source": [
    "## Dataset Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75215b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 7 CSV files in processed data:\n",
      "\n",
      "- student_entry_performance_eda.csv\n",
      "Size: 52.39 KB | Rows: 666 | Columns: 12\n",
      "\n",
      "- student_performance_binary_preprocessed.csv\n",
      "Size: 131.49 KB | Rows: 622 | Columns: 31\n",
      "\n",
      "- student_performance_cleaned.csv\n",
      "Size: 50.16 KB | Rows: 622 | Columns: 13\n",
      "\n",
      "- student_performance_encoded.csv\n",
      "Size: 50.16 KB | Rows: 622 | Columns: 13\n",
      "\n",
      "- student_performance_preprocessed.csv\n",
      "Size: 131.49 KB | Rows: 622 | Columns: 31\n",
      "\n",
      "- student_performance_test.csv\n",
      "Size: 26.96 KB | Rows: 125 | Columns: 31\n",
      "\n",
      "- student_performance_train.csv\n",
      "Size: 105.22 KB | Rows: 497 | Columns: 31\n",
      "\n",
      "================================================================================\n",
      "DATASET SUMMARY TABLE\n",
      "================================================================================\n",
      "                                   Filename Size (KB)  Rows  Columns\n",
      "          student_entry_performance_eda.csv     52.39   666       12\n",
      "student_performance_binary_preprocessed.csv    131.49   622       31\n",
      "            student_performance_cleaned.csv     50.16   622       13\n",
      "            student_performance_encoded.csv     50.16   622       13\n",
      "       student_performance_preprocessed.csv    131.49   622       31\n",
      "               student_performance_test.csv     26.96   125       31\n",
      "              student_performance_train.csv    105.22   497       31\n",
      "\n",
      "All datasets are stored in: /Users/lia/Desktop/Fase1/data/processed\n"
     ]
    }
   ],
   "source": [
    "# List all CSV files in processed directory\n",
    "csv_files = sorted(DATA_PROC.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"\\nFound {len(csv_files)} CSV files in processed data:\\n\")\n",
    "\n",
    "dataset_info = []\n",
    "for filepath in csv_files:\n",
    "    if filepath.exists():\n",
    "        size = filepath.stat().st_size\n",
    "        try:\n",
    "            df_temp = pd.read_csv(filepath)\n",
    "            rows, cols = df_temp.shape\n",
    "            dataset_info.append({\n",
    "                'Filename': filepath.name,\n",
    "                'Size (KB)': f\"{size/1024:.2f}\",\n",
    "                'Rows': rows,\n",
    "                'Columns': cols\n",
    "            })\n",
    "            print(f\"- {filepath.name}\")\n",
    "            print(f\"Size: {size/1024:.2f} KB | Rows: {rows} | Columns: {cols}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ {filepath.name} - Could not read: {e}\\n\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "if dataset_info:\n",
    "    df_inventory = pd.DataFrame(dataset_info)\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"DATASET SUMMARY TABLE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df_inventory.to_string(index=False))\n",
    "    print(f\"\\nAll datasets are stored in: {DATA_PROC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae0d9f",
   "metadata": {},
   "source": [
    "## Version History Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f36950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Git revision: 1fa2c28\n"
     ]
    }
   ],
   "source": [
    "# Helper function to get git revision\n",
    "def git_rev():\n",
    "    try:\n",
    "        rev = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],\n",
    "            stderr=subprocess.DEVNULL,\n",
    "            cwd=ROOT\n",
    "        ).decode('utf-8').strip()\n",
    "        return rev\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "git_revision = git_rev()\n",
    "print(f\"\\nGit revision: {git_revision}\")\n",
    "\n",
    "# Define version history\n",
    "version_entries = [\n",
    "    {\n",
    "        \"Version\": \"v1.0\",\n",
    "        \"Filename\": \"student_entry_performance_eda.csv\",\n",
    "        \"Description\": \"Original dataset after EDA with fixed column names\",\n",
    "        \"Changes\": \"Initial dataset – column names standardized, no other modifications.\"\n",
    "    },\n",
    "    {\n",
    "        \"Version\": \"v2.0\",\n",
    "        \"Filename\": \"student_performance_cleaned.csv\",\n",
    "        \"Description\": \"Cleaned data with duplicates removed + binary target\",\n",
    "        \"Changes\": \"Removed 44 duplicates (6.6%); created Performance_Binary (0=Lower, 1=High); verified no missing values.\"\n",
    "    },\n",
    "    {\n",
    "        \"Version\": \"v3.0\",\n",
    "        \"Filename\": \"student_performance_preprocessed.csv\",\n",
    "        \"Description\": \"Model-ready: encoded and scaled features\",\n",
    "        \"Changes\": \"Grouped rare study time categories; ordinal encoding for grades; one-hot encoding for nominal features; StandardScaler applied to ordinal features.\"\n",
    "    },\n",
    "    {\n",
    "        \"Version\": \"v3.1\",\n",
    "        \"Filename\": \"student_performance_train.csv\",\n",
    "        \"Description\": \"Training split (80% stratified)\",\n",
    "        \"Changes\": \"train_test_split with stratify; random_state=42; maintains class balance.\"\n",
    "    },\n",
    "    {\n",
    "        \"Version\": \"v3.2\",\n",
    "        \"Filename\": \"student_performance_test.csv\",\n",
    "        \"Description\": \"Test split (20% stratified)\",\n",
    "        \"Changes\": \"Hold-out set; not used in training; maintains class balance.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Get actual file stats\n",
    "def file_stats(filepath):\n",
    "    if not filepath.exists():\n",
    "        return {\"Rows\": \"—\", \"Columns\": \"—\", \"Size (KB)\": \"—\", \"Modified\": \"MISSING\"}\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        rows, cols = df.shape\n",
    "        size_kb = f\"{filepath.stat().st_size/1024:.2f}\"\n",
    "        mtime = datetime.fromtimestamp(filepath.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "        return {\"Rows\": rows, \"Columns\": cols, \"Size (KB)\": size_kb, \"Modified\": mtime}\n",
    "    except Exception as e:\n",
    "        return {\"Rows\": \"?\", \"Columns\": \"?\", \"Size (KB)\": \"?\", \"Modified\": f\"Error: {e.__class__.__name__}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5a2a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET VERSION CHANGELOG\n",
      "================================================================================\n",
      "\n",
      "v1.0: student_entry_performance_eda.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Description: Original dataset after EDA with fixed column names\n",
      "Shape: 666 rows × 12 columns | Size: 52.39 KB\n",
      "Modified: 2025-10-23 10:15\n",
      "\n",
      "Changes Made:\n",
      "Initial dataset – column names standardized, no other modifications.\n",
      "================================================================================\n",
      "\n",
      "v2.0: student_performance_cleaned.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Description: Cleaned data with duplicates removed + binary target\n",
      "Shape: 622 rows × 13 columns | Size: 50.16 KB\n",
      "Modified: 2025-10-23 10:35\n",
      "\n",
      "Changes Made:\n",
      "Removed 44 duplicates (6.6%); created Performance_Binary (0=Lower, 1=High); verified no missing values.\n",
      "================================================================================\n",
      "\n",
      "v3.0: student_performance_preprocessed.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Description: Model-ready: encoded and scaled features\n",
      "Shape: 622 rows × 31 columns | Size: 131.49 KB\n",
      "Modified: 2025-10-23 10:49\n",
      "\n",
      "Changes Made:\n",
      "Grouped rare study time categories; ordinal encoding for grades; one-hot encoding for nominal features; StandardScaler applied to ordinal features.\n",
      "================================================================================\n",
      "\n",
      "v3.1: student_performance_train.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Description: Training split (80% stratified)\n",
      "Shape: 497 rows × 31 columns | Size: 105.22 KB\n",
      "Modified: 2025-10-23 10:49\n",
      "\n",
      "Changes Made:\n",
      "train_test_split with stratify; random_state=42; maintains class balance.\n",
      "================================================================================\n",
      "\n",
      "v3.2: student_performance_test.csv\n",
      "--------------------------------------------------------------------------------\n",
      "Description: Test split (20% stratified)\n",
      "Shape: 125 rows × 31 columns | Size: 26.96 KB\n",
      "Modified: 2025-10-23 10:49\n",
      "\n",
      "Changes Made:\n",
      "Hold-out set; not used in training; maintains class balance.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "VERSION SUMMARY TABLE\n",
      "================================================================================\n",
      "Version                             Filename  Rows  Columns Size (KB)         Modified                                          Description\n",
      "   v1.0    student_entry_performance_eda.csv   666       12     52.39 2025-10-23 10:15   Original dataset after EDA with fixed column names\n",
      "   v2.0      student_performance_cleaned.csv   622       13     50.16 2025-10-23 10:35 Cleaned data with duplicates removed + binary target\n",
      "   v3.0 student_performance_preprocessed.csv   622       31    131.49 2025-10-23 10:49             Model-ready: encoded and scaled features\n",
      "   v3.1        student_performance_train.csv   497       31    105.22 2025-10-23 10:49                      Training split (80% stratified)\n",
      "   v3.2         student_performance_test.csv   125       31     26.96 2025-10-23 10:49                          Test split (20% stratified)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATASET VERSION CHANGELOG\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "records = []\n",
    "for item in version_entries:\n",
    "    fp = DATA_PROC / item[\"Filename\"]\n",
    "    stats = file_stats(fp)\n",
    "    \n",
    "    records.append({\n",
    "        \"Version\": item[\"Version\"],\n",
    "        \"Filename\": item[\"Filename\"],\n",
    "        \"Rows\": stats[\"Rows\"],\n",
    "        \"Columns\": stats[\"Columns\"],\n",
    "        \"Size (KB)\": stats[\"Size (KB)\"],\n",
    "        \"Modified\": stats[\"Modified\"],\n",
    "        \"Description\": item[\"Description\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{item['Version']}: {item['Filename']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Description: {item['Description']}\")\n",
    "    print(f\"Shape: {stats['Rows']} rows × {stats['Columns']} columns | Size: {stats['Size (KB)']} KB\")\n",
    "    print(f\"Modified: {stats['Modified']}\")\n",
    "    print(\"\\nChanges Made:\")\n",
    "    print(item[\"Changes\"])\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Create version summary table\n",
    "version_df = pd.DataFrame(records, columns=[\n",
    "    \"Version\", \"Filename\", \"Rows\", \"Columns\", \"Size (KB)\", \"Modified\", \"Description\"\n",
    "])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"VERSION SUMMARY TABLE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(version_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968cbe9",
   "metadata": {},
   "source": [
    "## Detailed Transformation Decumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba8e7b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRANSFORMATION 1: Data Cleaning (v1.0 → v2.0)\n",
      "================================================================================\n",
      "\n",
      "Operations Performed:\n",
      "1. Duplicate removal\n",
      "2. Binary target creation\n",
      "3. Data validation\n",
      "\n",
      "Code Implementation:\n",
      "\n",
      "# Remove duplicates\n",
      "df_clean = df.drop_duplicates()\n",
      "\n",
      "# Create binary target\n",
      "def create_binary_target(performance):\n",
      "    return 1 if performance in ['Excellent', 'Vg'] else 0\n",
      "\n",
      "df_clean['Performance_Binary'] = df_clean['Performance'].apply(create_binary_target)\n",
      "\n",
      "\n",
      "Impact:\n",
      "• Removed 44 rows (6.6%)\n",
      "• Added Performance_Binary; class balance → High: 44.4% / Lower: 55.6%\n",
      "• Shapes: v1=666 rows × 12 columns → v2=622 rows × 13 columns\n",
      "\n",
      "================================================================================\n",
      "TRANSFORMATION 2: Feature Engineering (v2.0 → v3.0)\n",
      "================================================================================\n",
      "\n",
      "Operations Performed:\n",
      "1. Rare category grouping\n",
      "2. Ordinal encoding (3 features)\n",
      "3. One-hot encoding (8 nominal variables)\n",
      "\n",
      "Code Implementation:\n",
      "\n",
      "# Group rare study time categories\n",
      "def group_study_time(time_val):\n",
      "    return 'FOUR_PLUS' if time_val in ['FOUR', 'FIVE', 'SEVEN'] else time_val\n",
      "\n",
      "# Ordinal mappings\n",
      "grade_mapping = {'Average': 1, 'Good': 2, 'Vg': 3, 'Excellent': 4}\n",
      "df_processed['Class_X_Grade_Encoded'] = df['Class_X_Percentage'].map(grade_mapping)\n",
      "\n",
      "# One-hot encoding\n",
      "df_encoded = pd.get_dummies(df, columns=nominal_columns, drop_first=True)\n",
      "\n",
      "\n",
      "Impact:\n",
      "• Features: 13 → 31 (+18)\n",
      "• Rows kept: 622 (should match v2)\n",
      "• Shapes: v2=622 rows × 13 columns → v3=622 rows × 31 columns\n",
      "\n",
      "================================================================================\n",
      "TRANSFORMATION 3: Train-Test Split (v3.0 → v3.1 & v3.2)\n",
      "================================================================================\n",
      "\n",
      "Operations Performed:\n",
      "  1. Stratified splitting (80/20)\n",
      "  2. Random state fixing (42)\n",
      "  3. Class balance verification\n",
      "\n",
      "Code Implementation:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
      ")\n",
      "\n",
      "\n",
      "Impact:\n",
      "• Train size: 497 | Test size: 125 (total 622)\n",
      "• Class balance (train): 0=55.5% / 1=44.5%\n",
      "• Class balance (test): 0=56.0% / 1=44.0%\n"
     ]
    }
   ],
   "source": [
    "# Load datasets if they exist\n",
    "def maybe_read(name):\n",
    "    fp = DATA_PROC / name\n",
    "    return (pd.read_csv(fp), fp) if fp.exists() else (None, fp)\n",
    "\n",
    "df_v1, fp_v1 = maybe_read(\"student_entry_performance_eda.csv\")\n",
    "df_v2, fp_v2 = maybe_read(\"student_performance_cleaned.csv\")\n",
    "df_v3, fp_v3 = maybe_read(\"student_performance_preprocessed.csv\")\n",
    "df_tr, fp_tr = maybe_read(\"student_performance_train.csv\")\n",
    "df_te, fp_te = maybe_read(\"student_performance_test.csv\")\n",
    "\n",
    "# Helper functions\n",
    "def shape_str(df):\n",
    "    return f\"{df.shape[0]} rows × {df.shape[1]} columns\" if df is not None else \"N/A\"\n",
    "\n",
    "def pct(x):\n",
    "    return f\"{100*x:.1f}%\"\n",
    "\n",
    "# Transformation 1: Data Cleaning (v1.0 → v2.0)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRANSFORMATION 1: Data Cleaning (v1.0 → v2.0)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cleaning_code = '''\n",
    "# Remove duplicates\n",
    "df_clean = df.drop_duplicates()\n",
    "\n",
    "# Create binary target\n",
    "def create_binary_target(performance):\n",
    "    return 1 if performance in ['Excellent', 'Vg'] else 0\n",
    "\n",
    "df_clean['Performance_Binary'] = df_clean['Performance'].apply(create_binary_target)\n",
    "'''\n",
    "\n",
    "print(\"\\nOperations Performed:\")\n",
    "print(\"1. Duplicate removal\")\n",
    "print(\"2. Binary target creation\")\n",
    "print(\"3. Data validation\")\n",
    "\n",
    "print(\"\\nCode Implementation:\")\n",
    "print(cleaning_code)\n",
    "\n",
    "if df_v1 is not None and df_v2 is not None:\n",
    "    dup_removed = df_v1.shape[0] - df_v2.shape[0]\n",
    "    dup_pct = pct(dup_removed / df_v1.shape[0]) if df_v1.shape[0] else \"0.0%\"\n",
    "    \n",
    "    if \"Performance_Binary\" in df_v2.columns:\n",
    "        bal = df_v2[\"Performance_Binary\"].value_counts(normalize=True).rename({0: \"Lower\", 1: \"High\"})\n",
    "        bal_str = \" / \".join([f\"{k}: {pct(v)}\" for k, v in bal.sort_index().items()])\n",
    "    else:\n",
    "        bal_str = \"Performance_Binary not found\"\n",
    "    \n",
    "    print(\"\\nImpact:\")\n",
    "    print(f\"• Removed {dup_removed} rows ({dup_pct})\")\n",
    "    print(f\"• Added Performance_Binary; class balance → {bal_str}\")\n",
    "    print(f\"• Shapes: v1={shape_str(df_v1)} → v2={shape_str(df_v2)}\")\n",
    "\n",
    "# Transformation 2: Feature Engineering (v2.0 → v3.0)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRANSFORMATION 2: Feature Engineering (v2.0 → v3.0)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "fe_code = '''\n",
    "# Group rare study time categories\n",
    "def group_study_time(time_val):\n",
    "    return 'FOUR_PLUS' if time_val in ['FOUR', 'FIVE', 'SEVEN'] else time_val\n",
    "\n",
    "# Ordinal mappings\n",
    "grade_mapping = {'Average': 1, 'Good': 2, 'Vg': 3, 'Excellent': 4}\n",
    "df_processed['Class_X_Grade_Encoded'] = df['Class_X_Percentage'].map(grade_mapping)\n",
    "\n",
    "# One-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=nominal_columns, drop_first=True)\n",
    "'''\n",
    "\n",
    "print(\"\\nOperations Performed:\")\n",
    "print(\"1. Rare category grouping\")\n",
    "print(\"2. Ordinal encoding (3 features)\")\n",
    "print(\"3. One-hot encoding (8 nominal variables)\")\n",
    "\n",
    "print(\"\\nCode Implementation:\")\n",
    "print(fe_code)\n",
    "\n",
    "if df_v2 is not None and df_v3 is not None:\n",
    "    print(\"\\nImpact:\")\n",
    "    print(f\"• Features: {df_v2.shape[1]} → {df_v3.shape[1]} (+{df_v3.shape[1]-df_v2.shape[1]})\")\n",
    "    print(f\"• Rows kept: {df_v3.shape[0]} (should match v2)\")\n",
    "    print(f\"• Shapes: v2={shape_str(df_v2)} → v3={shape_str(df_v3)}\")\n",
    "\n",
    "# Transformation 3: Train/Test Split (v3.0 → v3.1 & v3.2)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRANSFORMATION 3: Train-Test Split (v3.0 → v3.1 & v3.2)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "split_code = '''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"\\nOperations Performed:\")\n",
    "print(\"  1. Stratified splitting (80/20)\")\n",
    "print(\"  2. Random state fixing (42)\")\n",
    "print(\"  3. Class balance verification\")\n",
    "\n",
    "print(\"\\nCode Implementation:\")\n",
    "print(split_code)\n",
    "\n",
    "target_col = \"Performance_Binary\"\n",
    "if df_tr is not None and df_te is not None:\n",
    "    n_tr, n_te = df_tr.shape[0], df_te.shape[0]\n",
    "    print(\"\\nImpact:\")\n",
    "    print(f\"• Train size: {n_tr} | Test size: {n_te} (total {n_tr + n_te})\")\n",
    "    \n",
    "    if target_col in df_tr.columns and target_col in df_te.columns:\n",
    "        bal_tr = df_tr[target_col].value_counts(normalize=True).sort_index()\n",
    "        bal_te = df_te[target_col].value_counts(normalize=True).sort_index()\n",
    "        print(\"• Class balance (train): \" + \" / \".join([f\"{int(k)}={pct(v)}\" for k, v in bal_tr.items()]))\n",
    "        print(\"• Class balance (test): \" + \" / \".join([f\"{int(k)}={pct(v)}\" for k, v in bal_te.items()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c9671",
   "metadata": {},
   "source": [
    "## Create DVC Setup Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e36b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instructions saved to: /Users/lia/Desktop/Fase1/DVC_SETUP_INSTRUCTIONS.md\n"
     ]
    }
   ],
   "source": [
    "dvc_instructions = f\"\"\"\n",
    "DVC (Data Version Control) Setup Instructions\n",
    "==============================================\n",
    "\n",
    "Objectives:\n",
    "- Track ONLY RAW data with DVC (e.g., 'data/raw/')\n",
    "- Generate processed data/models/figures via pipeline ('dvc.yaml' outs)\n",
    "- Never 'dvc add' processed files\n",
    "\n",
    "Project root: {ROOT}\n",
    "\n",
    "Step 0: One-time Git init (if needed)\n",
    "--------------------------------------\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"init repo\"\n",
    "\n",
    "Step 1: Initialize DVC\n",
    "----------------------\n",
    "Use the helper script you created (portable & repeatable):\n",
    "\n",
    "./scripts/init_dvc.sh\n",
    "\n",
    "(Equivalent to `dvc init`, configure, and commit .dvc files.)\n",
    "\n",
    "Step 2: Track ONLY raw data with DVC\n",
    "-----------------------------------\n",
    "Place the original dataset(s) in `data/raw/` and run:\n",
    "\n",
    "./scripts/track_data_dvc.sh\n",
    "\n",
    "This executes:\n",
    "- dvc add data/raw\n",
    "- git add data/raw.dvc\n",
    "- git commit -m \"track raw data with DVC\"\n",
    "\n",
    "Step 3: Define and Run the pipeline\n",
    "--------------------------------\n",
    "The `dvc.yaml` stage runs preprocessing and declares outs:\n",
    "\n",
    "dvc repro\n",
    "\n",
    "On each run, it (re)creates:\n",
    "- data/processed/*.csv\n",
    "- models/ (if produced)\n",
    "- reports/figures/ (plots)\n",
    "\n",
    "Step 4: Check status & push artifacts\n",
    "----------------------------\n",
    "dvc status\n",
    "git add dvc.yaml dvc.lock\n",
    "git commit -m \"pipeline run\"\n",
    "dvc push\n",
    "\n",
    "(If you haven't set a remote, the init script added a local one: ../dvcstore)\n",
    "\n",
    "Common Commands\n",
    "---------------\n",
    "# Check what changed\n",
    "dvc status\n",
    "\n",
    "# Pull data from remote\n",
    "dvc pull\n",
    "\n",
    "# Push data to remote\n",
    "dvc push\n",
    "\n",
    "# Reproduce pipeline\n",
    "dvc repro\n",
    "\n",
    "# Show pipeline DAG\n",
    "dvc dag\n",
    "\n",
    "Benefits of DVC\n",
    "===============\n",
    "✓ Version control for large datasets\n",
    "✓ Reproducibility of data pipeline\n",
    "✓ Efficient storage (only metadata in Git)\n",
    "✓ Easy collaboration with team members\n",
    "✓ Data integrity verification with MD5 hashes\n",
    "\"\"\"\n",
    "\n",
    "instruction_file = ROOT / \"DVC_SETUP_INSTRUCTIONS.md\"\n",
    "instruction_file.write_text(dvc_instructions, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\nInstructions saved to: {instruction_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166494ba",
   "metadata": {},
   "source": [
    "## Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bfe7c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA VERSIONING SUMMARY - COMPLETE\n",
      "================================================================================\n",
      "\n",
      "1) DVC Implementation\n",
      "   • DVC initialized and linked to Git\n",
      "   • Raw data tracked with `dvc add data/raw/`\n",
      "   • Processed data/figures are pipeline outs in `dvc.yaml`\n",
      "   • Reproducibility anchored by Git commit + `dvc.lock`\n",
      "   • Current Git rev: 1fa2c28\n",
      "\n",
      "2) Documentation of Data Modifications\n",
      "   • Version history: v1.0 → v3.2\n",
      "   • Each transformation explained with rationale and code snippets\n",
      "   • Shapes computed from actual artifacts in `data/processed/`\n",
      "\n",
      "3) Change Log / History\n",
      "   v1.0 (Original/EDA): 666 × 12\n",
      "   ↓ [Remove duplicates, create binary target]\n",
      "   v2.0 (Cleaned): 622 × 13\n",
      "   ↓ [Encode categorical (ordinal + one-hot)]\n",
      "   v3.0 (Preprocessed): 622 × 31\n",
      "   ↓ [80/20 stratified split]\n",
      "   v3.1 (Train): 497 × 31\n",
      "   v3.2 (Test): 125 × 31\n",
      "\n",
      "Key Metrics\n",
      "-----------\n",
      "• Total duplicate rows removed: 44\n",
      "• Percentage removed: 6.6%\n",
      "• Feature expansion: 13 → 31 (+18)\n",
      "• Final class balance (train): 0=55.5% / 1=44.5%\n",
      "• Final class balance (test): 0=56.0% / 1=44.0%\n",
      "• Target column: Performance_Binary\n",
      "• Random seed: 42\n",
      "\n",
      "Files Created\n",
      "-------------\n",
      "1. scripts/init_dvc.sh - DVC initialization script\n",
      "2. scripts/track_data_dvc.sh - Data tracking script\n",
      "3. dvc.yaml - Pipeline configuration\n",
      "4. params.yaml - Parameter file\n",
      "5. DVC_SETUP_INSTRUCTIONS.md - Setup guide\n",
      "\n",
      "Next Steps\n",
      "----------\n",
      "→ Run: ./scripts/init_dvc.sh (initialize DVC)\n",
      "→ Run: ./scripts/track_data_dvc.sh (track raw data)\n",
      "→ Run: dvc repro (reproduce pipeline)\n",
      "→ Proceed to Notebook 04: Model Training with MLflow\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Summary saved to: /Users/lia/Desktop/Fase1/DATA_VERSIONING_SUMMARY.md\n",
      "\n",
      "================================================================================\n",
      "DATA VERSIONING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Artifacts Created:\n",
      "  ✓ scripts/init_dvc.sh\n",
      "  ✓ scripts/track_data_dvc.sh\n",
      "  ✓ dvc.yaml\n",
      "  ✓ params.yaml\n",
      "  ✓ DVC_SETUP_INSTRUCTIONS.md\n",
      "  ✓ DATA_VERSIONING_SUMMARY.md\n"
     ]
    }
   ],
   "source": [
    "# Compute dynamic metrics\n",
    "def shape(df):\n",
    "    return (df.shape[0], df.shape[1]) if df is not None else (None, None)\n",
    "\n",
    "r1, c1 = shape(df_v1)\n",
    "r2, c2 = shape(df_v2)\n",
    "r3, c3 = shape(df_v3)\n",
    "rt, ct = shape(df_tr)\n",
    "re, ce = shape(df_te)\n",
    "\n",
    "# Duplicates removed\n",
    "dup_removed = (r1 - r2) if (r1 is not None and r2 is not None) else None\n",
    "dup_pct = (100*dup_removed/r1) if (dup_removed is not None and r1) else None\n",
    "\n",
    "# Feature expansion\n",
    "feat_from = c2 if c2 is not None else c1\n",
    "feat_to = c3 if c3 is not None else 0\n",
    "feat_delta = (feat_to - feat_from) if (feat_from is not None and feat_to is not None) else None\n",
    "\n",
    "# Class balance\n",
    "def balance_str(df, target=\"Performance_Binary\"):\n",
    "    if df is None or target not in df.columns:\n",
    "        return \"N/A\"\n",
    "    vc = df[target].value_counts(normalize=True).sort_index()\n",
    "    return \" / \".join([f\"{int(k)}={v*100:.1f}%\" for k, v in vc.items()])\n",
    "\n",
    "bal_train = balance_str(df_tr)\n",
    "bal_test = balance_str(df_te)\n",
    "\n",
    "def fmt_shape(r, c):\n",
    "    return f\"{r} × {c}\" if (r is not None and c is not None) else \"MISSING\"\n",
    "\n",
    "summary = f\"\"\"\n",
    "{'='*80}\n",
    "DATA VERSIONING SUMMARY - COMPLETE\n",
    "{'='*80}\n",
    "\n",
    "1) DVC Implementation\n",
    "   • DVC initialized and linked to Git\n",
    "   • Raw data tracked with `dvc add data/raw/`\n",
    "   • Processed data/figures are pipeline outs in `dvc.yaml`\n",
    "   • Reproducibility anchored by Git commit + `dvc.lock`\n",
    "   • Current Git rev: {git_revision}\n",
    "\n",
    "2) Documentation of Data Modifications\n",
    "   • Version history: v1.0 → v3.2\n",
    "   • Each transformation explained with rationale and code snippets\n",
    "   • Shapes computed from actual artifacts in `data/processed/`\n",
    "\n",
    "3) Change Log / History\n",
    "   v1.0 (Original/EDA): {fmt_shape(r1, c1)}\n",
    "   ↓ [Remove duplicates, create binary target]\n",
    "   v2.0 (Cleaned): {fmt_shape(r2, c2)}\n",
    "   ↓ [Encode categorical (ordinal + one-hot)]\n",
    "   v3.0 (Preprocessed): {fmt_shape(r3, c3)}\n",
    "   ↓ [80/20 stratified split]\n",
    "   v3.1 (Train): {fmt_shape(rt, ct)}\n",
    "   v3.2 (Test): {fmt_shape(re, ce)}\n",
    "\n",
    "Key Metrics\n",
    "-----------\n",
    "• Total duplicate rows removed: {dup_removed if dup_removed is not None else 'N/A'}\n",
    "• Percentage removed: {f'{dup_pct:.1f}%' if dup_pct is not None else 'N/A'}\n",
    "• Feature expansion: {feat_from} → {feat_to} ({'+'+str(feat_delta) if feat_delta else 'N/A'})\n",
    "• Final class balance (train): {bal_train}\n",
    "• Final class balance (test): {bal_test}\n",
    "• Target column: Performance_Binary\n",
    "• Random seed: 42\n",
    "\n",
    "Files Created\n",
    "-------------\n",
    "1. scripts/init_dvc.sh - DVC initialization script\n",
    "2. scripts/track_data_dvc.sh - Data tracking script\n",
    "3. dvc.yaml - Pipeline configuration\n",
    "4. params.yaml - Parameter file\n",
    "5. DVC_SETUP_INSTRUCTIONS.md - Setup guide\n",
    "\n",
    "Next Steps\n",
    "----------\n",
    "→ Run: ./scripts/init_dvc.sh (initialize DVC)\n",
    "→ Run: ./scripts/track_data_dvc.sh (track raw data)\n",
    "→ Run: dvc repro (reproduce pipeline)\n",
    "→ Proceed to Notebook 04: Model Training with MLflow\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "summary_file = ROOT / \"DATA_VERSIONING_SUMMARY.md\"\n",
    "summary_file.write_text(summary, encoding=\"utf-8\")\n",
    "print(f\"\\nSummary saved to: {summary_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATA VERSIONING COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(\"\\nArtifacts Created:\")\n",
    "artifacts = [\n",
    "    SCRIPTS / \"init_dvc.sh\",\n",
    "    SCRIPTS / \"track_data_dvc.sh\",\n",
    "    ROOT / \"dvc.yaml\",\n",
    "    ROOT / \"params.yaml\",\n",
    "    ROOT / \"DVC_SETUP_INSTRUCTIONS.md\",\n",
    "    ROOT / \"DATA_VERSIONING_SUMMARY.md\"\n",
    "]\n",
    "\n",
    "for artifact in artifacts:\n",
    "    status = \"✓\" if artifact.exists() else \"✗\"\n",
    "    print(f\"  {status} {artifact.relative_to(ROOT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b44c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
